# Berkeley-Videos

Here I'll be sharing usefull resources and my implementations of the subjects covered in the [CS 198-126: Deep Learning for Visual Data course](https://ml-berkeley.notion.site/CS-198-126-Deep-Learning-for-Visual-Data-a57e2aca54c046edb7014439f81ba1d5).

---

Index:

- [ ] Transformers
- [ ] CLIP
- [ ] Diffusion models
- [ ] GANs
- [ ] Auto Encoders
- [ ] Vision Transformers
- [ ] CNNs
- [ ] Multimodal Transformers

---
# Useful tools + General resources:

* [Weights and Biases:](https://wandb.ai/site/)  Weights & Biases (W&B) is a platform designed to help machine learning practitioners track experiments, visualize metrics, manage datasets, and collaborate more effectively.

---
# Miscellaneous:

**Microsoft's OmniParser**: An advanced tool designed for extracting and interpreting interactive elements within graphical user interfaces (GUIs), like app screens or website interfaces. Primarily aimed at enhancing multimodal language models, such as GPT-4 Vision, OmniParser converts GUI screenshots into structured data by identifying clickable icons, text fields, and other interactable regions, and by associating relevant semantic information with each identified element. 

You can find the demo and model card on HuggingFace: https://huggingface.co/spaces/microsoft/OmniParser

The results I got using the model demo:
![ ](
